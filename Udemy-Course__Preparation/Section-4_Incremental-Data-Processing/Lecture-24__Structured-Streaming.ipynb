{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 24. Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You will learn\n",
    "\n",
    "- what is a data stream and how to process streaming data using spark structured streaming.\n",
    "- how to use a data stream reader to perform a stream read from a source and \n",
    "- how to use and configure a data stream writer to perform a streaming write to sink."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datastream\n",
    "\n",
    "A ***data stream*** is any data source that grows over time.\n",
    "For example, \n",
    "  - a new data in a data stream might correspond to a new JSON log file landing into a cloud storage, \n",
    "  - updates to a database captured in a CDC or Change Data Capture feed\n",
    "  - events queued in a pub/sub messaging feed like Kafka.\n",
    "\n",
    "### Processing Data Stream\n",
    "\n",
    "To process a data stream, usually you have two approaches: \n",
    "  - either a traditional approach where you reprocess the entire dataset each time you receive a new update to your data.\n",
    "  - Or another approach would be to write a custom logic to only capture those files or records that have been added since the last time an update was run.\n",
    "    And here we can use the spark structured streaming to achieve this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Structured Streaming\n",
    "\n",
    "**Spark structured streaming** is a scalable streaming processing engine.\n",
    "It allows you to query an infinite data source where automatically detects new data and persists the result incrementally into a data sink.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../../assets/images/Spark Structured Streaming.jpg\" style=\"width:640px\" >\n",
    "</div> \n",
    "\n",
    "A ***data sink*** is just a durable file system, such as files or tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating Infinite Data as a Table\n",
    "\n",
    "But the question is how to interact and query an infinite data source !?\n",
    "\n",
    "Simply by treating it as a table.\n",
    "\n",
    "In fact, the magic behind Spark Structured Streaming is that it allows users to interact with ever-growing data source as if it were just a static table of records.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../../assets/images/Treating Infinite Data as a Table.jpg\" style=\"width:640px\" >\n",
    "</div> \n",
    "\n",
    "So new data in the input data stream is simply treated as new rows appended to a table.\n",
    "\n",
    "And such a table representing an infinite data source is seen as \"unbounded\" table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Streaming Table\n",
    "\n",
    "As we said, an input data stream could be a directory of files, a messaging system like Kafka, or simply a Delta table.\n",
    "\n",
    "Delta Lake is well integrated with spark structured streaming.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../../assets/images/Input Streaming Table.jpg\" style=\"width:640px\" >\n",
    "</div> \n",
    "\n",
    "We can simply use `spark.readStream()` to query the delta table as a stream source, which allows to process all of the data present in the table as well as any new data that arrive later.\n",
    "\n",
    "```python\n",
    "streamDF = spark.readStream\n",
    "                .table(\"Input_Table\")\n",
    "```\n",
    "This creates a streaming data frame on which we can apply any transformation as if it were just a static data frame.\n",
    "\n",
    "Then to persist the result of a streaming query, we need to write them out to durable storage using `dataframe.writeStream` method.\n",
    "With the `writeStream` method, we can configure our output.\n",
    "\n",
    "```python\n",
    "streamDF.writeStream\n",
    "        .trigger(processingTime=\"2 minutes\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", \"/path\")\n",
    "        .table(\"Output_Table\")\n",
    "```\n",
    "Here for example, we trigger the streaming processing every 2 minutes to check if there are new arriving records, and we choose to append them to the target table.\n",
    "\n",
    "So again, after another 2 minutes, we will check if there are new arriving data and we append them to the target table.\n",
    "\n",
    "And all this happened thanks to checkpoints created by Spark to track the progress of your streaming processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger Intervals\n",
    "\n",
    "Let us take a closer look on these configurations.\n",
    "\n",
    "When defining a streaming write, the trigger method specifies when the system should process the next set of data.\n",
    "And this is called the ***trigger interval***.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../../assets/images/Trigger Intervals.jpg\" style=\"width:640px\" >\n",
    "</div> \n",
    "\n",
    "By default, if you don't provide any trigger interval, the data will be processed every half second.\n",
    "\n",
    "Or you can specify a fixed interval as we did previously.\n",
    "So the data will be processed in micro batches at your specified interval, for example, every 5 minutes.\n",
    "\n",
    "In addition, you can run your stream in a batch mode to process all available data at once using either trigger `once` option, or `availableNow` option.\n",
    "\n",
    "In both cases, the trigger will stop on its own once finished processing the available data.\n",
    "\n",
    "The only difference is that with the trigger Once, all available data will be processed in a single batch, compared to multiple micro batches in availableNow option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Modes\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../../assets/images/writeStream Output Modes.jpg\" style=\"width:640px\" >\n",
    "</div> \n",
    "\n",
    "Streaming jobs have also output mode similar to static workloads, either: \n",
    "- `append` mode, which is the default mode.\n",
    "\n",
    "  In this mode, only new appended rows are incrementally appended to the target table with each batch.\n",
    "\n",
    "- While in `complete` mode,\n",
    "\n",
    "  the result table is recalculated each time a write is triggered, so the target table is overwritten with each batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing\n",
    "\n",
    "Databricks creates checkpoints by storing the current state of your streaming job to cloud storage.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../../assets/images/writeStream Checkpointing.jpg\" style=\"width:640px\" >\n",
    "</div> \n",
    "\n",
    "Checkpointings allow the streaming engine to track the progress of your streaming processing.\n",
    "\n",
    "An important note here is that checkpoints cannot be shared between several streams.\n",
    "\n",
    "A separate checkpoint location is required for every streaming write to ensure processing guarantees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guarantees\n",
    "\n",
    "Structured streaming provides two guarantees.\n",
    "\n",
    "1. Fault Tolerance\n",
    "   \n",
    "    First in case of failure, the streaming agent can resume from where it left off.\n",
    "\n",
    "    Thanks to both the checkpointing and also a mechanism called *Write-ahead logs*, they allow to record the offset range of data being processed during each trigger interval, to track your stream progress.\n",
    "\n",
    "2. Exactly-once guarantee\n",
    "\n",
    "    Structured streaming also ensures exactly once data processing because the streaming sinks are designed to be idempotent.\n",
    "\n",
    "    That is, multiple writes of the same data, of course identified by the offset, do not result in duplicates being written to the sink.\n",
    "\n",
    "And of course, the two guarantees here only work if the streaming source is repeatable, like cloud based object storage or pub/sub messaging service.\n",
    "\n",
    "So, taking all together, repeatable data sources and idempotent sinks allows spark structured streaming \n",
    "to ensure end-to-end exactly-once semantics under any failure condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupported Operations\n",
    "\n",
    "Lastly, we need to understand that some operations are not supported by streaming data frames.\n",
    "\n",
    "Yes, it is true that most operations on a streaming data frame are identical to a static data frame, but there are some exceptions to this.\n",
    "\n",
    "Operations such as **sorting** and **deduplication**, are either too complex or logically not possible to do when working with streaming data.\n",
    "A full discussion of this exception is out of scope of this course.\n",
    "\n",
    "However, there are advanced streaming methods like **windowing** and **watermarking** that can help to do such operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
