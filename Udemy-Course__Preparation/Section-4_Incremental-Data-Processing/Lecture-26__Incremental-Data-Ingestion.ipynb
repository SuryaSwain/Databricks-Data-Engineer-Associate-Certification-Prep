{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lecture 26. Incremental Data Ingestion from Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "  - [Documentation > Ingest data into a Databricks lakehouse > Ingest data from cloud object storage > What is Auto Loader?](https://docs.databricks.com/ingestion/auto-loader/index.html)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Incremental Data Ingestion?\n",
    "\n",
    "By ***incremental data ingestion***, we mean the ability to *load data from new files that have been encountered since the last ingestion*. \n",
    "\n",
    "So each time we run our data pipeline, we don't need to reprocess the files we have processed before. We need to process only the new arriving data files.  (*Reduces redundant processing*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for Incremental Data Ingestion\n",
    "\n",
    "Databricks provides two mechanisms for incrementally and efficiently processing new data files as they arrive in a storage location, which are `COPY INTO` SQL command and Auto Loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `COPY INTO` Command\n",
    "\n",
    "`COPY INTO` is a **SQL command** that allows users to load data from a file location into a Delta table. It loads data *idempotently* and *incrementally*. It means each time you run the command, it will **load only the new files** from the source location, while *the files that have been loaded before are simply skipped*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format\n",
    "\n",
    "The command is pretty simple: \n",
    "\n",
    "```sql\n",
    "COPY INTO my_table\n",
    "FROM '/path/to/files’\n",
    "FILEFORMAT = <format>\n",
    "FORMAT_OPTIONS (<format options>)\n",
    "COPY_OPTIONS (<copy options>);\n",
    "```\n",
    "\n",
    "- `COPY INTO <target_table> FROM <source_location>`\n",
    "- And we specify the format of the source file to load, for example, `CSV` or `parquet`, \n",
    "- and any related format options, \n",
    "- in addition to any option to control the operation of the `COPY INTO` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Here, for example, we are loading from *CSV files*, having a header and a specific delimiter. And in the `COPY OPTIONS`, we are specifying that the schema can evolve according to the incoming data.\n",
    "\n",
    "```sql\n",
    "COPY INTO my_table\n",
    "FROM '/path/to/files'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('delimiter' = '|',\n",
    "                'header' = 'true')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Loader\n",
    "\n",
    "The second method to load data incrementally from files is ***Auto Loader***, which uses *structured streaming* in Spark to efficiently process new data files as they arrive in a storage location. \n",
    "You can use Auto Loader to *load billions of files into a table*.\n",
    "Auto Loader can scale to support near real-time ingestion of millions of files per hour. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto loader Checkpointing\n",
    "\n",
    "And since it is based on *Spark Structured Streaming*, Auto Loader uses *checkpointing* to track the ingestion process and to store metadata of the discovered files. \n",
    "\n",
    "So Auto Loader ensures that data files are processed *exactly once*. \n",
    "\n",
    "And in case of failure, Auto Loader can resume from where it left off. (Fault tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto Loader in PySpark API\n",
    "\n",
    "To work with Auto Loader, we use the `readStream` and `writeStream` methods. \n",
    "\n",
    "```python\n",
    "spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", <source_format>)\n",
    "        .load('/path/to/files’)\n",
    "     .writeStream\n",
    "        .option(\"checkpointLocation\", <checkpoint_directory>)\n",
    "        .table(<table_name>)\n",
    "```\n",
    "\n",
    "Auto Loader has a specific format for *Stream Reader* called `cloudFiles`. \n",
    "\n",
    "And in order to specify the format of the source files, we simply use the `cloudFiles.format` option. \n",
    "\n",
    "And with the `load` function, we provide the location of the source files.\n",
    "In this location, Auto Loader will detect new files as they arrive and queue them for ingestion. \n",
    "\n",
    "Once we read the files, we write the data into a target table using the sream writer, where we provide the location to store the checkpointing information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto Loader + Schema\n",
    "\n",
    "Auto Loader can automatically infer the schema of your data. \n",
    "It can detect any update to the fields of the source dataset. \n",
    "\n",
    "However, to avoid this *inference cost* at every startup of your stream, the inferred schema can be stored in a location to be used later.\n",
    "\n",
    "```python\n",
    "spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", <source_format>)\n",
    "        .option(\"cloudFiles.schemaLocation\", <schema_directory>)\n",
    "        .load('/path/to/files’)\n",
    "     .writeStream\n",
    "        .option(\"checkpointLocation\", <checkpoint_directory>)\n",
    "        .option(\"mergeSchema\", “true”)\n",
    "        .table(<table_name>)\n",
    "```\n",
    "\n",
    "To provide the location where Auto Loader can store the schema, use the option: `cloudFiles.schemaLocation`\n",
    "\n",
    "And this location could be simply the same as the checkpoint location.\n",
    "\n",
    "\n",
    "## When to Use Auto Loader vs. `COPY INTO`\n",
    "\n",
    "Now we know everything about Auto Loader. The question is, when to use Auto Loader and when to use `COPY INTO` command?\n",
    "\n",
    "Here are important things to consider when choosing between `COPY INTO` and Auto Loader:\n",
    "\n",
    "- If you are going to ingest files in the order of *thousands*, you can use the `COPY INTO` command.\n",
    "- However, if you are expecting files in the order of *millions* or more over time, use Auto Loader.\n",
    "\n",
    "In addition, Auto Loader can split the processing into multiple batches, so it is more efficient at scale. Databricks recommends using Auto Loader as a general best practice when ingesting data from *cloud object storage*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Let us now switch to the Databricks platform to [work with Auto Loader](./Lecture-27__Auto-Loader-(Hands-On).ipynb).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
