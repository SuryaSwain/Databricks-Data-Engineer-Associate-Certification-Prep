{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88a3a2d3-d9ae-428a-a1f1-96e3102aad81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Lecture 25. Structured Streaming (Hands On)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdfbff42-62d7-47f2-b610-687ed07c1bbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this notebook, we will explore the basics of working with Spark Structured Streaming to allow incremental processing of data.\n",
    "\n",
    "We will continue using our bookstore dataset that contains Customers, Orders and Books tables.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../../assets/images/bookstore_schema.png\" style=\"width:640px\" >\n",
    "</div> \n",
    "\n",
    "Let us first copy our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4bd2b96-601a-4c7d-b756-22b715456e6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Copy-Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "699d46d6-6fa2-4341-9695-31b1e0b2215c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading Stream\n",
    "\n",
    "To work with data streaming in SQL, you must first use `spark.readStream` method PySpark API.\n",
    "This is why we are using here a Python notebook.\n",
    "\n",
    "`spark.readStream` method allows to query a Delta table as a stream source.\n",
    "And from there, we can register a temporary view against this stream source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4629eb3-5059-437a-97b4-64c3bfea2300",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    "      .table(\"books\")\n",
    "      .createOrReplaceTempView(\"books_streaming_tmp_vw\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e8e2431-faba-4490-8086-2de8ad6f0c1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The temporary view created here is a \"streaming\" temporary view that allows to apply most transformations in SQL the same way as we would with the static data.\n",
    "\n",
    "### Displaying Streaming Data\n",
    "Let us first query this streaming temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f302dc7c-c0c0-4e31-a37f-8ec33e03ccbe",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>book_id</th><th>title</th><th>author</th><th>category</th><th>price</th></tr></thead><tbody><tr><td>B14</td><td>Data Communications and Networking</td><td>Behrouz A. Forouzan</td><td>Computer Science</td><td>34.0</td></tr><tr><td>B15</td><td>Inside the Java Virtual Machine</td><td>Bill Venners</td><td>Computer Science</td><td>41.0</td></tr><tr><td>B13</td><td>Linux pocket guide</td><td>Daniel J. Barrett</td><td>Computer Science</td><td>26.0</td></tr><tr><td>B10</td><td>Beginning Database Design Solutions</td><td>Rod Stephens</td><td>Computer Science</td><td>44.0</td></tr><tr><td>B11</td><td>Business Intelligence for Dummies</td><td>Swain Scheps</td><td>Computer Science</td><td>38.0</td></tr><tr><td>B12</td><td>Big Data in Practice</td><td>Bernard Marr</td><td>Computer Science</td><td>30.0</td></tr><tr><td>B01</td><td>The Soul of a New Machine</td><td>Tracy Kidder</td><td>Computer Science</td><td>49.0</td></tr><tr><td>B02</td><td>Learning JavaScript Design Patterns</td><td>Addy Osmani</td><td>Computer Science</td><td>28.0</td></tr><tr><td>B03</td><td>Make Your Own Neural Network</td><td>Tariq Rashid</td><td>Computer Science</td><td>35.0</td></tr><tr><td>B07</td><td>The Hundred-Page Machine Learning</td><td>Andriy Burkov</td><td>Computer Science</td><td>33.0</td></tr><tr><td>B08</td><td>Quantum Computing for Everyone</td><td>Chris Bernhardt</td><td>Computer Science</td><td>41.0</td></tr><tr><td>B09</td><td>Advanced Data Structures</td><td>Peter Brass</td><td>Computer Science</td><td>24.0</td></tr><tr><td>B04</td><td>Robot Dynamics and Control</td><td>Mark W. Spong</td><td>Computer Science</td><td>20.0</td></tr><tr><td>B05</td><td>Fluent Python</td><td>Luciano Ramalho</td><td>Computer Science</td><td>47.0</td></tr><tr><td>B06</td><td>Deep Learning with Python</td><td>François Chollet</td><td>Computer Science</td><td>22.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "B14",
         "Data Communications and Networking",
         "Behrouz A. Forouzan",
         "Computer Science",
         34.0
        ],
        [
         "B15",
         "Inside the Java Virtual Machine",
         "Bill Venners",
         "Computer Science",
         41.0
        ],
        [
         "B13",
         "Linux pocket guide",
         "Daniel J. Barrett",
         "Computer Science",
         26.0
        ],
        [
         "B10",
         "Beginning Database Design Solutions",
         "Rod Stephens",
         "Computer Science",
         44.0
        ],
        [
         "B11",
         "Business Intelligence for Dummies",
         "Swain Scheps",
         "Computer Science",
         38.0
        ],
        [
         "B12",
         "Big Data in Practice",
         "Bernard Marr",
         "Computer Science",
         30.0
        ],
        [
         "B01",
         "The Soul of a New Machine",
         "Tracy Kidder",
         "Computer Science",
         49.0
        ],
        [
         "B02",
         "Learning JavaScript Design Patterns",
         "Addy Osmani",
         "Computer Science",
         28.0
        ],
        [
         "B03",
         "Make Your Own Neural Network",
         "Tariq Rashid",
         "Computer Science",
         35.0
        ],
        [
         "B07",
         "The Hundred-Page Machine Learning",
         "Andriy Burkov",
         "Computer Science",
         33.0
        ],
        [
         "B08",
         "Quantum Computing for Everyone",
         "Chris Bernhardt",
         "Computer Science",
         41.0
        ],
        [
         "B09",
         "Advanced Data Structures",
         "Peter Brass",
         "Computer Science",
         24.0
        ],
        [
         "B04",
         "Robot Dynamics and Control",
         "Mark W. Spong",
         "Computer Science",
         20.0
        ],
        [
         "B05",
         "Fluent Python",
         "Luciano Ramalho",
         "Computer Science",
         47.0
        ],
        [
         "B06",
         "Deep Learning with Python",
         "François Chollet",
         "Computer Science",
         22.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "book_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "author",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM books_streaming_tmp_vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a810a122-0425-412d-ad1e-1d9ffa320066",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here, what we see is a streaming result. \n",
    "As you can see, the query is still running, waiting for any new data to be displayed here.\n",
    "\n",
    "Generally speaking, we don't display a streaming result unless a human is actively monitoring the output of a query during development or live dashboarding.\n",
    "\n",
    "Let us click `Interrupt` to stop this active streaming query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "491b0f0a-934b-4343-9886-2add853673ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Applying Transformations\n",
    "\n",
    "Let us now apply some aggregations on this streaming temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af437972-d9f9-430e-8749-c212c6b93798",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>author</th><th>total_books</th></tr></thead><tbody><tr><td>Mark W. Spong</td><td>1</td></tr><tr><td>Chris Bernhardt</td><td>1</td></tr><tr><td>Tariq Rashid</td><td>1</td></tr><tr><td>Peter Brass</td><td>1</td></tr><tr><td>Luciano Ramalho</td><td>1</td></tr><tr><td>Addy Osmani</td><td>1</td></tr><tr><td>Andriy Burkov</td><td>1</td></tr><tr><td>Tracy Kidder</td><td>1</td></tr><tr><td>Swain Scheps</td><td>1</td></tr><tr><td>François Chollet</td><td>1</td></tr><tr><td>Daniel J. Barrett</td><td>1</td></tr><tr><td>Rod Stephens</td><td>1</td></tr><tr><td>Behrouz A. Forouzan</td><td>1</td></tr><tr><td>Bernard Marr</td><td>1</td></tr><tr><td>Bill Venners</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Mark W. Spong",
         1
        ],
        [
         "Chris Bernhardt",
         1
        ],
        [
         "Tariq Rashid",
         1
        ],
        [
         "Peter Brass",
         1
        ],
        [
         "Luciano Ramalho",
         1
        ],
        [
         "Addy Osmani",
         1
        ],
        [
         "Andriy Burkov",
         1
        ],
        [
         "Tracy Kidder",
         1
        ],
        [
         "Swain Scheps",
         1
        ],
        [
         "François Chollet",
         1
        ],
        [
         "Daniel J. Barrett",
         1
        ],
        [
         "Rod Stephens",
         1
        ],
        [
         "Behrouz A. Forouzan",
         1
        ],
        [
         "Bernard Marr",
         1
        ],
        [
         "Bill Venners",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "author",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_books",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT author, count(book_id) AS total_books\n",
    "FROM books_streaming_tmp_vw\n",
    "GROUP BY author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f77dfe4-2b7c-4079-b99d-f680ebd55106",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Again, because we are querying a streaming temporary view,\n",
    "this becomes a streaming query that executes infinitely, \n",
    "rather than completing after retrieving a single set of results.\n",
    "\n",
    "And here we are just displaying an aggregation of input as seen by the stream.\n",
    "None of these records are being persisted anywhere at this point.\n",
    "\n",
    "For streaming queries like this, we can always explore an interactive dashboard to monitor the streaming performance.\n",
    "\n",
    "Before continuing, let us cancel this active streaming query by clicking `Interrupt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "853c28b4-8336-4f85-8428-3a6a3ffd1c31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Unsupported Operations\n",
    "\n",
    "Remember, when working with streaming data, some operations are not supported like sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d51c984a-457b-4ed2-a3d7-b4b337de8e0f",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "com.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode; line 3 pos 0;\n",
       "~Sort [author#24 ASC NULLS FIRST], true\n",
       "+- ~Project [book_id#22, title#23, author#24, category#25, price#26]\n",
       "   +- ~SubqueryAlias books_streaming_tmp_vw\n",
       "      +- View (`books_streaming_tmp_vw`, [book_id#22,title#23,author#24,category#25,price#26])\n",
       "         +- ~SubqueryAlias hive_metastore.default.books\n",
       "            +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3d266da0,delta,List(),None,List(),None,Map(path -> *********(redacted)),Some(CatalogTable(\n",
       "Catalog: hive_metastore\n",
       "Database: default\n",
       "Table: books\n",
       "Owner: root\n",
       "Created Time: Mon Oct 14 02:54:09 UTC 2024\n",
       "Last Access: UNKNOWN\n",
       "Created By: Spark 3.4.1\n",
       "Type: MANAGED\n",
       "Provider: delta\n",
       "Table Properties: [delta.lastCommitTimestamp=1728874447000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\n",
       "Location: dbfs:/user/hive/warehouse/books\n",
       "Serde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n",
       "InputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\n",
       "OutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n",
       "Partition Provider: Catalog\n",
       "Schema: root\n",
       "-- book_id: string (nullable = true)\n",
       "-- title: string (nullable = true)\n",
       "-- author: string (nullable = true)\n",
       "-- category: string (nullable = true)\n",
       "-- price: double (nullable = true)\n",
       "))), tahoe, [book_id#22, title#23, author#24, category#25, price#26]\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.throwError(UnsupportedOperationChecker.scala:594)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.$anonfun$checkForStreaming$5(UnsupportedOperationChecker.scala:527)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.$anonfun$checkForStreaming$5$adapted(UnsupportedOperationChecker.scala:301)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:248)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:247)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:247)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
       "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:247)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:247)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:247)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
       "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:247)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:301)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:56)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:39)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:39)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:41)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:286)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:286)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:283)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:266)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:353)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:353)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:353)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:233)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:434)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:427)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:353)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:225)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:225)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:405)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:402)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:404)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:193)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:383)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:451)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:965)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:451)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:447)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:447)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:187)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:186)\n",
       "\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:323)\n",
       "\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:416)\n",
       "\tat org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:509)\n",
       "\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:427)\n",
       "\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:272)\n",
       "\tat com.databricks.backend.daemon.driver.DriverSparkHooks.startMemoryStream(DriverSparkHooks.scala:147)\n",
       "\tat com.databricks.backend.daemon.driver.DriverSparkHooks.displayStreamingDataFrame(DriverSparkHooks.scala:176)\n",
       "\tat com.databricks.backend.daemon.driver.DisplayDataFrameDataset.display(DisplayDataFrameDataset.scala:30)\n",
       "\tat com.databricks.backend.daemon.driver.DisplayDataFrameDataset.display$(DisplayDataFrameDataset.scala:17)\n",
       "\tat com.databricks.backend.daemon.driver.EnhancedRDDFunctions$.display(EnhancedRDDFunctions.scala:22)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:233)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:361)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:714)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$35(DriverLocal.scala:1018)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:1009)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:72)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:72)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:964)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1397)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:660)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:859)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:851)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:891)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:667)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:685)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:76)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:662)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:891)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:702)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:803)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:577)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:577)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:487)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:296)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:379)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:714)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$35(DriverLocal.scala:1018)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:1009)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:72)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:72)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:964)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1397)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:660)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:859)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:851)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:891)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:667)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:685)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:76)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:662)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:891)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:702)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:803)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:577)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:577)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:487)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:296)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "com.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode; line 3 pos 0;\n~Sort [author#24 ASC NULLS FIRST], true\n+- ~Project [book_id#22, title#23, author#24, category#25, price#26]\n   +- ~SubqueryAlias books_streaming_tmp_vw\n      +- View (`books_streaming_tmp_vw`, [book_id#22,title#23,author#24,category#25,price#26])\n         +- ~SubqueryAlias hive_metastore.default.books\n            +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3d266da0,delta,List(),None,List(),None,Map(path -> *********(redacted)),Some(CatalogTable(\nCatalog: hive_metastore\nDatabase: default\nTable: books\nOwner: root\nCreated Time: Mon Oct 14 02:54:09 UTC 2024\nLast Access: UNKNOWN\nCreated By: Spark 3.4.1\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1728874447000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nLocation: dbfs:/user/hive/warehouse/books\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n |-- book_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- author: string (nullable = true)\n |-- category: string (nullable = true)\n |-- price: double (nullable = true)\n))), tahoe, [book_id#22, title#23, author#24, category#25, price#26]\n\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.throwError(UnsupportedOperationChecker.scala:594)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.$anonfun$checkForStreaming$5(UnsupportedOperationChecker.scala:527)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.$anonfun$checkForStreaming$5$adapted(UnsupportedOperationChecker.scala:301)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:248)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:247)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:247)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:247)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:247)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:247)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:247)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:301)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:56)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:41)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:39)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:39)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:41)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:40)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:286)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:286)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:283)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:266)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:353)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:353)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:233)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:434)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:353)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:225)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:225)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:405)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:402)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:404)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:193)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:383)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:451)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:965)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:451)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:447)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:447)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:187)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:186)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:323)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:416)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:509)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:427)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:272)\n\tat com.databricks.backend.daemon.driver.DriverSparkHooks.startMemoryStream(DriverSparkHooks.scala:147)\n\tat com.databricks.backend.daemon.driver.DriverSparkHooks.displayStreamingDataFrame(DriverSparkHooks.scala:176)\n\tat com.databricks.backend.daemon.driver.DisplayDataFrameDataset.display(DisplayDataFrameDataset.scala:30)\n\tat com.databricks.backend.daemon.driver.DisplayDataFrameDataset.display$(DisplayDataFrameDataset.scala:17)\n\tat com.databricks.backend.daemon.driver.EnhancedRDDFunctions$.display(EnhancedRDDFunctions.scala:22)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:233)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:361)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:714)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$35(DriverLocal.scala:1018)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:1009)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:72)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:72)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:964)\n\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1397)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:660)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:859)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:851)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:891)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:667)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:685)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:76)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:662)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:891)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:702)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:803)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:577)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:577)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:487)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:296)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:379)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:714)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$35(DriverLocal.scala:1018)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:1009)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:72)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:72)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:964)\n\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1397)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:660)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:859)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:851)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:891)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:667)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:685)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:76)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:662)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:891)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:702)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:803)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:577)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:577)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:487)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:296)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "AnalysisException: Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode; line 3 pos 0;\n~Sort [author#24 ASC NULLS FIRST], true\n+- ~Project [book_id#22, title#23, author#24, category#25, price#26]\n   +- ~SubqueryAlias books_streaming_tmp_vw\n      +- View (`books_streaming_tmp_vw`, [book_id#22,title#23,author#24,category#25,price#26])\n         +- ~SubqueryAlias hive_metastore.default.books\n            +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3d266da0,delta,List(),None,List(),None,Map(path -> *********(redacted)),Some(CatalogTable(\nCatalog: hive_metastore\nDatabase: default\nTable: books\nOwner: root\nCreated Time: Mon Oct 14 02:54:09 UTC 2024\nLast Access: UNKNOWN\nCreated By: Spark 3.4.1\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1728874447000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nLocation: dbfs:/user/hive/warehouse/books\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n |-- book_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- author: string (nullable = true)\n |-- category: string (nullable = true)\n |-- price: double (nullable = true)\n))), tahoe, [book_id#22, title#23, author#24, category#25, price#26]\n",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM books_streaming_tmp_vw\n",
    "ORDER BY author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3edc7691-85cf-427e-b99a-0f921a98ef98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can use advanced methods like windowing and watermarking to achieve such operations, but it is out of scope for this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06d3d3c9-0c8c-43a7-a104-6439f5c131a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Persisting Streaming Data\n",
    "\n",
    "Now, in order to persist incremental results, we need first to pass our logic back to PySpark DataFrame API.\n",
    "\n",
    "Here we are creating another temporary view.\n",
    "And since we are creating this temporary view from the result of a query against a streaming temporary view, so this new temporary view is also a \"streaming\" temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f4fc98e-b3f7-4e7d-9035-1df2080b5683",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW author_counts_tmp_vw AS (\n",
    "  SELECT author, count(book_id) AS total_books\n",
    "  FROM books_streaming_tmp_vw\n",
    "  GROUP BY author\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6ed6884-8d81-4922-878e-35c089c153eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The new streaming temporary view has been created.\n",
    "\n",
    "In PySpark DataFrame API, we can use the `spark.table()` to load data from a streaming temporary view back to a DataFrame.\n",
    "\n",
    "Note that spark always loads streaming views as a streaming DataFrames, and static views as a static DataFrames, meaning that: incremental processing must be defined from the very beginning with Read logic to support later an incremental writing.\n",
    "\n",
    "Then, we are using DataFrame `writeStream` method to persist the result of a streaming query to a durable storage.\n",
    "This allows us to configure the output with the three settings: \n",
    "- The trigger intervals, here every 4 seconds. \n",
    "- The output mode, either append or complete.\n",
    "- For aggregation streaming queries, we must always use \"complete\" mode to overwrite the table with the new calculation.\n",
    "- Finally, the checkpoint location to help tracking the progress of the streaming processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "283e6cee-b078-4820-922d-e96fc1c48351",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f1ba4687970>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(spark.table(\"author_counts_tmp_vw\")                               \n",
    "      .writeStream  \n",
    "      .trigger(processingTime='4 seconds')\n",
    "      .outputMode(\"complete\")\n",
    "      .option(\"checkpointLocation\", \"dbfs:/mnt/demo/author_counts_checkpoint\")\n",
    "      .table(\"author_counts\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a278aad-7160-4042-bf04-79d010da909e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can think about such a streaming query as an always-on incremental query, and we can always explore its interactive dashboard.\n",
    "From this dashboard, we can see that the data has been processed and we can now query our target table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e11634f-c1c9-4c80-8f24-8e75dae01828",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>author</th><th>total_books</th></tr></thead><tbody><tr><td>Behrouz A. Forouzan</td><td>1</td></tr><tr><td>François Chollet</td><td>1</td></tr><tr><td>Daniel J. Barrett</td><td>1</td></tr><tr><td>Luciano Ramalho</td><td>1</td></tr><tr><td>Chris Bernhardt</td><td>1</td></tr><tr><td>Andriy Burkov</td><td>1</td></tr><tr><td>Mark W. Spong</td><td>1</td></tr><tr><td>Bill Venners</td><td>1</td></tr><tr><td>Tariq Rashid</td><td>1</td></tr><tr><td>Bernard Marr</td><td>1</td></tr><tr><td>Rod Stephens</td><td>1</td></tr><tr><td>Swain Scheps</td><td>1</td></tr><tr><td>Tracy Kidder</td><td>1</td></tr><tr><td>Peter Brass</td><td>1</td></tr><tr><td>Addy Osmani</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Behrouz A. Forouzan",
         1
        ],
        [
         "François Chollet",
         1
        ],
        [
         "Daniel J. Barrett",
         1
        ],
        [
         "Luciano Ramalho",
         1
        ],
        [
         "Chris Bernhardt",
         1
        ],
        [
         "Andriy Burkov",
         1
        ],
        [
         "Mark W. Spong",
         1
        ],
        [
         "Bill Venners",
         1
        ],
        [
         "Tariq Rashid",
         1
        ],
        [
         "Bernard Marr",
         1
        ],
        [
         "Rod Stephens",
         1
        ],
        [
         "Swain Scheps",
         1
        ],
        [
         "Tracy Kidder",
         1
        ],
        [
         "Peter Brass",
         1
        ],
        [
         "Addy Osmani",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 13
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "author",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_books",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM author_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "498897f3-2c67-4478-980e-44f916f69a17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Our data has been written to the target table, the `author_counts` table, and we can see that each author has currently only 1 book.\n",
    "\n",
    "And remember, what you see here is not a \"streaming\" query! simply because we are querying the table directly.\n",
    "I mean, not as a streaming source through a streaming DataFrame.\n",
    "\n",
    "And if we come back to our streaming query, we see that it is still active. \n",
    "In fact, when we execute a streaming query, the streaming query will continue to update as new data arrives in the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d32f8a8-6f88-4750-9039-dcb5656ee98a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Adding New Data\n",
    "\n",
    "To confirm this, let us add new data to our source table, the `books` table. Let us run this query and see what will happen in our streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b96e038-27cb-4db4-a626-e412395ca17f",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>3</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 14
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "INSERT INTO books\n",
    "values (\"B19\", \"Introduction to Modeling and Simulation\", \"Mark W. Spong\", \"Computer Science\", 25),\n",
    "        (\"B20\", \"Robot Modeling and Control\", \"Mark W. Spong\", \"Computer Science\", 30),\n",
    "        (\"B21\", \"Turing's Vision: The Birth of Computer Science\", \"Chris Bernhardt\", \"Computer Science\", 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b03123fd-d995-4fbf-873d-b9f1934af1ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that there is a new date arriving.\n",
    "\n",
    "Let us query our target table again to see the updated books counts for each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2f7cf0-8e39-4b2e-b48e-17ccca93fe60",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>author</th><th>total_books</th></tr></thead><tbody><tr><td>Behrouz A. Forouzan</td><td>1</td></tr><tr><td>François Chollet</td><td>1</td></tr><tr><td>Daniel J. Barrett</td><td>1</td></tr><tr><td>Chris Bernhardt</td><td>2</td></tr><tr><td>Luciano Ramalho</td><td>1</td></tr><tr><td>Mark W. Spong</td><td>3</td></tr><tr><td>Andriy Burkov</td><td>1</td></tr><tr><td>Tariq Rashid</td><td>1</td></tr><tr><td>Rod Stephens</td><td>1</td></tr><tr><td>Tracy Kidder</td><td>1</td></tr><tr><td>Bernard Marr</td><td>1</td></tr><tr><td>Swain Scheps</td><td>1</td></tr><tr><td>Bill Venners</td><td>1</td></tr><tr><td>Addy Osmani</td><td>1</td></tr><tr><td>Peter Brass</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Behrouz A. Forouzan",
         1
        ],
        [
         "François Chollet",
         1
        ],
        [
         "Daniel J. Barrett",
         1
        ],
        [
         "Chris Bernhardt",
         2
        ],
        [
         "Luciano Ramalho",
         1
        ],
        [
         "Mark W. Spong",
         3
        ],
        [
         "Andriy Burkov",
         1
        ],
        [
         "Tariq Rashid",
         1
        ],
        [
         "Rod Stephens",
         1
        ],
        [
         "Tracy Kidder",
         1
        ],
        [
         "Bernard Marr",
         1
        ],
        [
         "Swain Scheps",
         1
        ],
        [
         "Bill Venners",
         1
        ],
        [
         "Addy Osmani",
         1
        ],
        [
         "Peter Brass",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 15
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "author",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_books",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM author_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4f581c6-09ab-4d43-a89f-865981d9890e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we see some authors having more than 1 book.\n",
    "\n",
    "Let us come back to our streaming query, and cancel it to see another scenario.\n",
    "\n",
    "Always remember to cancel any active stream in your notebook, otherwise the stream will be always on and prevents the cluster from auto termination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "810016b4-4abb-4cbf-ae36-b413eb902832",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Streaming in Batch Mode\n",
    "\n",
    "For our last scenario, we will add some books for new authors to our source table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72573233-bc6b-46fa-ae8d-56a03a9170c8",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>3</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 16
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "INSERT INTO books\n",
    "values (\"B16\", \"Hands-On Deep Learning Algorithms with Python\", \"Sudharsan Ravichandiran\", \"Computer Science\", 25),\n",
    "       (\"B17\", \"Neural Network Methods in Natural Language Processing\", \"Yoav Goldberg\", \"Computer Science\", 30),\n",
    "       (\"B18\", \"Understanding digital signal processing\", \"Richard Lyons\", \"Computer Science\", 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73a2e0cd-5eda-4cc3-a34e-82c23572832e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Three records have been inserted.\n",
    "\n",
    "In this scenario, we modify the trigger method to change our query from an always-on query triggered every 4 seconds to a triggered incremental batch.\n",
    "\n",
    "We do this using the `availableNow` trigger option.\n",
    "With this trigger option, the query will process all new available data and stop on its own after execution.\n",
    "\n",
    "In this case, we can use the `awaitTermination` method to block the execution of any cell in this notebook until the incremental batch's write has succeeded.\n",
    "\n",
    "Let us now run this query to process the three records we have just added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "477b9864-0787-402f-871a-be5c56d616d1",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "(spark.table(\"author_counts_tmp_vw\")                               \n",
    "      .writeStream           \n",
    "      .trigger(availableNow=True)\n",
    "      .outputMode(\"complete\")\n",
    "      .option(\"checkpointLocation\", \"dbfs:/mnt/demo/author_counts_checkpoint\")\n",
    "      .table(\"author_counts\")\n",
    "      .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0b0dcfd-9737-4f67-ad4a-e5b1aca11b01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As you can see, with the `availableNow` trigger option, the query runs in a batch mode.\n",
    "\n",
    "It is executed to process all the available data and then stop on its own.\n",
    "\n",
    "Let us finally query the target table again to see the updated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d8864a5-6cce-4c6d-88d2-71df3b0dc95d",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>author</th><th>total_books</th></tr></thead><tbody><tr><td>Sudharsan Ravichandiran</td><td>1</td></tr><tr><td>Behrouz A. Forouzan</td><td>1</td></tr><tr><td>François Chollet</td><td>1</td></tr><tr><td>Daniel J. Barrett</td><td>1</td></tr><tr><td>Luciano Ramalho</td><td>1</td></tr><tr><td>Chris Bernhardt</td><td>2</td></tr><tr><td>Andriy Burkov</td><td>1</td></tr><tr><td>Yoav Goldberg</td><td>1</td></tr><tr><td>Richard Lyons</td><td>1</td></tr><tr><td>Mark W. Spong</td><td>3</td></tr><tr><td>Bernard Marr</td><td>1</td></tr><tr><td>Tariq Rashid</td><td>1</td></tr><tr><td>Bill Venners</td><td>1</td></tr><tr><td>Swain Scheps</td><td>1</td></tr><tr><td>Rod Stephens</td><td>1</td></tr><tr><td>Tracy Kidder</td><td>1</td></tr><tr><td>Peter Brass</td><td>1</td></tr><tr><td>Addy Osmani</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Sudharsan Ravichandiran",
         1
        ],
        [
         "Behrouz A. Forouzan",
         1
        ],
        [
         "François Chollet",
         1
        ],
        [
         "Daniel J. Barrett",
         1
        ],
        [
         "Luciano Ramalho",
         1
        ],
        [
         "Chris Bernhardt",
         2
        ],
        [
         "Andriy Burkov",
         1
        ],
        [
         "Yoav Goldberg",
         1
        ],
        [
         "Richard Lyons",
         1
        ],
        [
         "Mark W. Spong",
         3
        ],
        [
         "Bernard Marr",
         1
        ],
        [
         "Tariq Rashid",
         1
        ],
        [
         "Bill Venners",
         1
        ],
        [
         "Swain Scheps",
         1
        ],
        [
         "Rod Stephens",
         1
        ],
        [
         "Tracy Kidder",
         1
        ],
        [
         "Peter Brass",
         1
        ],
        [
         "Addy Osmani",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 18
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "author",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_books",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM author_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b7affb5-838b-467e-987a-1f7078ec126b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Now we have 18 authors instead of 15."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3594856130581070,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Lecture-25__Structured-Streaming-(Hands-On)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
