{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 38: Certification Overview\n",
    "\n",
    "In this video, we will see an overview of the certification exam.\n",
    "\n",
    "  - You will learn about the format and structure behind the exam.\n",
    "  - Describe the topics covered in the exam.\n",
    "  - And lastly, recognize the different types of questions provided on the exam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam Details\n",
    "\n",
    "- You have **90 minutes** to complete the certification exam.\n",
    "\n",
    "- There are **45 multiple choice questions**.\n",
    "\n",
    "- And your score must be **70%** or better.\n",
    "\n",
    "  This translates to correctly answering a minimum of **32 out of 45 questions**.\n",
    "\n",
    "- Each attempt of the certification exam will cost you **$200**.\n",
    "\n",
    "  And you will be able to retake the exam as many times as you would like. But you will need to pay **$200** for each attempt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam Topics\n",
    "\n",
    "The questions expected during the exam will be distributed in the following way:\n",
    "\n",
    "- **11 questions** out of 45 on the use and the benefits of using the **Databricks Lakehouse platform**.\n",
    "- **13 questions** on **building ETL pipelines** with **Apache Spark SQL** and **Python**.\n",
    "- **10 questions** on **processing data incrementally**.\n",
    "- **7 questions** on building **production pipelines**, in addition to **Databricks SQL queries** and **Dashboards**.\n",
    "- Lastly, **4 questions** on **data governance and security practices**.\n",
    "\n",
    "We have fully covered all these topics during our course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Not Required for the Exam\n",
    "\n",
    "The following topics are **not** expected on the Associate-level Data Engineer exam:\n",
    "\n",
    "- All topics related to **Spark internals**, **Databricks CLI** and **API**, **Change Data Capture**, **modeling concepts**, and **data protection regulations**.\n",
    "- Same for **monitoring and logging production jobs**, **dependency management**, and **testing**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding and Exam Platform\n",
    "\n",
    "### Coding\n",
    "\n",
    "During the exam, **data manipulation code** will always be provided in **SQL** when possible.\n",
    "\n",
    "In all other cases, the code will be in **Python**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exam Platform\n",
    "\n",
    "**Databricks certifications** are taken through the **Webassessor platform** via this link (https://www.webassessor.com/databricks).\n",
    "\n",
    "You just need to sign up in order to schedule and take your exam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exam Proctoring\n",
    "\n",
    "- During the exam, you will be monitored via webcam by a **Webassessor Proctor**.\n",
    "\n",
    "- You will be asked to provide a **valid photo-based identification**.\n",
    "\n",
    "- The proctor will monitor you during the exam and can provide technical support if needed.\n",
    "\n",
    "  However, they will not provide assistance on the content of the exam.\n",
    "\n",
    "- In addition, there will be **no test aids** during the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exam Results and Certification\n",
    "\n",
    "The certification exams are automatically graded, and you will receive your pass or fail grade immediately.\n",
    "\n",
    "However, the **badge** and **certificate** will be received within **24 hours** of passing the exam.\n",
    "\n",
    "You will receive them via: https://credentials.databricks.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Questions\n",
    "\n",
    "Remember, all the exam questions are **multiple choice questions**.\n",
    "This means there is only **one correct answer** for each question.\n",
    "\n",
    "The exam has two types of questions: **conceptual** or **code-based** questions.\n",
    "\n",
    "  - **Conceptual questions** usually ask for definitions.\n",
    "\n",
    "    Example: *What something is? or does?*\n",
    "\n",
    "    **Question example:** Which part of the Databricks Lakehouse platform can data engineers use to orchestrate jobs?\n",
    "\n",
    "    - **Options:**\n",
    "      - Repos\n",
    "      - Workflows\n",
    "      - Data Explorer\n",
    "      - Databricks SQL\n",
    "      - Cluster\n",
    "\n",
    "    **Answer:** Workflows.\n",
    "\n",
    "    In the **Workflows** tab, you can create and orchestrate jobs in Databricks.\n",
    "\n",
    "  - **Code-based questions** could ask you, for example, to identify an **error** in a code block or to **complete** a code block by filling in the blanks.\n",
    "\n",
    "    **Question example:** Here, you have a code block of a **stream write operation** with a missing configuration.\n",
    "\n",
    "    ```python\n",
    "    spark.table(\"sales\") .writeStream\n",
    "    .option(\"checkpointLocation\", checkpointPath)\n",
    "    ._____\n",
    "    .table(\"new_sales\")\n",
    "    ```\n",
    "\n",
    "    The question asks:\n",
    "\n",
    "    If you want the query to execute a single micro-batch to process all of the available data, which of the following lines of code should you use to fill in the blank?\n",
    "\n",
    "    - `trigger(once=True)`\n",
    "    - `trigger(continuous=\"once\")`\n",
    "    - `processingTime(\"once\")`\n",
    "    - `trigger(processingTime=\"once\")`\n",
    "    - `processingTime(1)`\n",
    "\n",
    "    **Answer:** `Trigger.Once`\n",
    "\n",
    "    With `Trigger.Once`, you execute a **single micro-batch** to process all of the available data, as we saw in the **Structured Streaming lecture**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Test\n",
    "\n",
    "Lastly, **Databricks** offers a **practice test** for the Associate-Level Data Engineer exam.\n",
    "\n",
    "This practice test is available in [**PDF format**](../../assets/resources/PracticeExam_DatabricksDataEngineerAssociate.pdf).\n",
    "\n",
    "Let us take a look at it.\n",
    "\n",
    "Here is the **official practice test** from Databricks.\n",
    "You have **45 questions**.\n",
    "\n",
    "These questions are representative of the questions that are on the actual exam, but they are no longer on the actual exam.\n",
    "\n",
    "Once you have completed the practice exam, evaluate your score using the **correct answers** at the end of this document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Advice\n",
    "\n",
    "My **last advice** before appearing for the exam is to try to **practice by yourself** all the notebooks that we saw during the course.\n",
    "\n",
    "I wish you all the best in your certification exam, and please let me know about your results via **LinkedIn**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
