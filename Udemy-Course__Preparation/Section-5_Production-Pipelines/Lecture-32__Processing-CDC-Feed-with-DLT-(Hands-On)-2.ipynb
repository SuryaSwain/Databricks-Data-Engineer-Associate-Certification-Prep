{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 32. Processing CDC Feed with DLT (Hands On) - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Notebook to Be Added to the Pipeline\n",
    "\n",
    "### Bronze Layer Tables\n",
    "\n",
    "We start by creating a **bronze table** to ingest the **book CDC feed**.  \n",
    "  We are using **auto loader** to load the JSON files incrementally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE books_bronze\n",
    "COMMENT \"The raw books data, ingested from CDC feed\"\n",
    "AS SELECT * FROM cloud_files(\"${dataset.path}/books-cdc\", \"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver Layer Tables\n",
    "\n",
    "Next, we create the **silver table**.  \n",
    "This is our **target table**, into which the changes from the CDC feed will be applied.  \n",
    "We start by declaring the table since `Apply Changes Into` requires the target table to be declared in a separate statement.\n",
    "\n",
    "Now, with the target table created, we can write the `Apply Changes Into` command.\n",
    "  - In this command, we specify the table `book_silver` as the **target table**,  and the table `books_bronze` as the **streaming source** of our CDC feed.\n",
    "\n",
    "  - Then, we identify the `book_id` as the **primary key**.  \n",
    "    - If the key exists in the target table, the record will be **updated**.  \n",
    "    - If not, it will be **inserted**.\n",
    "\n",
    "  - Next, we specify that records where the `row_status` is **\"delete\"** should be **deleted** from the target table.\n",
    "\n",
    "  - And we specify the `row_time` field for **ordering the operations**.\n",
    "\n",
    "  - Lastly, we indicate that all **books fields** should be added to the target table, except the operational columns:  \n",
    "`row_status` and `row_time`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE books_silver;\n",
    "\n",
    "APPLY CHANGES INTO LIVE.books_silver\n",
    "  FROM STREAM(LIVE.books_bronze)\n",
    "  KEYS (book_id)\n",
    "  APPLY AS DELETE WHEN row_status = \"DELETE\"\n",
    "  SEQUENCE BY row_time\n",
    "  COLUMNS * EXCEPT (row_status, row_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold Layer Table\n",
    "\n",
    "At the **gold layer**, we define a simple **aggregate query** to create a live table from the data in our `book_silver` table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE LIVE TABLE author_counts_state\n",
    "  COMMENT \"Number of books per author\"\n",
    "AS SELECT author, count(*) as books_count, current_timestamp() updated_time\n",
    "  FROM LIVE.books_silver\n",
    "  GROUP BY author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that this is **not a streaming table**.  \n",
    "Since data is being updated and deleted from our `book_silver` table, it is no longer valid to be a **streaming source** for this new table.  \n",
    "Remember, streaming sources must be **append-only** tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a DLT View\n",
    "\n",
    "Lastly, we see here how to define a **DLT view**.\n",
    "\n",
    "In DLT pipelines, we can also define **views**. To define a view, simply replace `TABLE` with the `VIEW` keyword.\n",
    "\n",
    "**DLT views** are **temporary views** scoped to the DLT pipeline they are a part of, so they are not persisted to the **metastore**.  \n",
    "Views can still be used to enforce **data quality**.  \n",
    "And **metrics for views** will be collected and reported as they would be for tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE LIVE VIEW books_sales\n",
    "  AS SELECT b.title, o.quantity\n",
    "    FROM (\n",
    "      SELECT *, explode(books) AS book \n",
    "      FROM LIVE.orders_cleaned) o\n",
    "    INNER JOIN LIVE.books_silver b\n",
    "    ON o.book.book_id = b.book_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining and Referencing Tables Across Notebooks\n",
    "\n",
    "Here, we see how we can **join** and reference tables across notebooks.\n",
    "\n",
    "We are joining our `book_silver` table to the `orders_cleaned` table, which we created in another notebook in the last lecture.\n",
    "\n",
    "Since **DLT** supports scheduling multiple notebooks as part of a single **DLT pipeline configuration**, code in any notebook can reference tables and views created in any other notebook.\n",
    "\n",
    "Essentially, you can think of the scope of the schema referenced by the **LIVE** keyword to be at the Delta pipeline level, rather than the individual notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Adding a New Notebook to the Existing DLT Pipeline](./Lecture-32__Processing-CDC-Feed-with-DLT-(Hands-On)-3.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
