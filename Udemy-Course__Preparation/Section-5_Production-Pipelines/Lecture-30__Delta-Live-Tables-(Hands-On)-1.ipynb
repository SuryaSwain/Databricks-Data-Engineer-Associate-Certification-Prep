{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 30. Delta Live Tables (Hands On)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "- Azure Subscription, remaining quota should be 4+ cores.\n",
    "- The Demo Cluster should be exist with the running results of the prior notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Delta Live Tables Overview\n",
    "\n",
    "Delta Live Tables or **DLT** is a framework for building reliable and maintainable data processing pipelines.  \n",
    "DLT simplifies the hard work of building large-scale ETL while maintaining table dependencies and data quality.\n",
    "\n",
    "---\n",
    "\n",
    "<div  style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../assets/images/Screen-Captures/Workflows - Delta Live Tables - Pipeline details.jpg\" alt=\"Workflows - Delta Live Tables - Pipeline details.jpg\" style=\"width: 1280px\">\n",
    "</div>\n",
    "\n",
    "Here, our **DLT** multi-hop pipeline is well visualized, and we can see our two bronze tables, `customers` and `orders_raw`.  \n",
    "They are joined together into the silver table `orders_cleaned`, from which we calculate our gold table `daily_customer_books`.\n",
    "\n",
    "DLT pipelines are implemented using Databricks notebooks. On the pipeline details on the right, we can see the path to the notebook containing the DLT table definitions.  \n",
    "We can simply click here to navigate to the source code.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Live Tables Syntax and Table Declaration\n",
    "\n",
    "Let us explore the content of this notebook to better understand the syntax used by Delta Live Tables.\n",
    "\n",
    "In this SQL notebook, we declare our **Delta Live Tables** that together implement a simple multi-hop architecture.  \n",
    "**DLT tables** will always be preceded by the `LIVE` keyword.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "<div  style=\"text-align: center; \">\n",
    "  <img src=\"../../assets/images/Presentation-Images/bookstore_schema.png\" alt=\"Raw Data Schema\" style=\"width: 480px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SET datasets.path=dbfs:/mnt/demo-datasets/bookstore;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bronze Layer Tables\n",
    "\n",
    "Here, we start by declaring two tables implementing the **bronze layer**.  \n",
    "These represent our data in its rawest form.\n",
    "\n",
    "#### `orders_raw`\n",
    "\n",
    "The table `orders_raw` ingests **Parquet** data incrementally by **Auto Loader** from our dataset directory.  \n",
    "Incremental processing via **Auto Loader** requires the addition of the `STREAMING` keyword in the declaration.\n",
    "\n",
    "The `cloud_files` method enables Auto Loader to be used natively with SQL.  \n",
    "This method takes three parameters:\n",
    "- The data file source location.\n",
    "- The source data format, which is `parquet` in this case.\n",
    "- An array of **Reader options**.\n",
    "\n",
    "In this case, we declare the schema of our data.  \n",
    "Also, notice that we add a comment here that will be visible to anyone exploring the data catalog.\n",
    "\n",
    "Let us run this query and see what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING LIVE TABLE orders_raw\n",
    "COMMENT \"The raw books orders, ingested from orders-raw\"\n",
    "AS SELECT * FROM cloud_files(\"${datasets.path}/orders-json-raw\", \"json\",\n",
    "                             map(\"cloudFiles.inferColumnTypes\", \"true\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, running a **DLT** query from here only validates that it is syntactically valid.  \n",
    "To define and populate this table, you must create a **DLT pipeline**.  \n",
    "We will see later how to configure and run a new pipeline from this notebook.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `customers`\n",
    "\n",
    "The second **bronze table** is `customers`, which presents **JSON** customer data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH LIVE TABLE customers\n",
    "COMMENT \"The customers lookup table, ingested from customers-json\"\n",
    "AS SELECT * FROM json.`${datasets.path}/customers-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table is used below in a **join** operation to look up customer information.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver Layer Tables\n",
    "\n",
    "Next, we declare tables implementing the **silver layer**.  \n",
    "This layer represents a refined copy of data from the **bronze layer**.\n",
    "\n",
    "At this level, we apply operations like **data cleansing** and **enrichment**.\n",
    "\n",
    "#### `orders_cleaned`\n",
    "\n",
    "Here we declare our silver table `orders_cleaned`, which enriches the order's data with customer information.  \n",
    "\n",
    "In addition, we implement **quality control** using `CONSTRAINT` keywords.\n",
    "Here, we reject records with no `order_id`.  \n",
    "The `CONSTRAINT` keyword enables **DLT** to collect metrics on constraint violations.  \n",
    "It provides an optional `ON VIOLATION` clause specifying an action to take on records that violate the constraints.\n",
    "\n",
    "The three modes currently supported by **Delta** are included in this table:\n",
    "- `DROP ROW`, where we discard records that violate constraints.\n",
    "- `FAIL UPDATE`, where the pipeline fails when a constraint is violated.\n",
    "- Finally, when omitted, records violating constraints will be included, but violations will be reported in the metrics.\n",
    "\n",
    ">> Constraint violation\n",
    "\n",
    "| **`ON VIOLATION`** | Behavior |\n",
    "| --- | --- |\n",
    "| **`DROP ROW`** | Discard records that violate constraints |\n",
    "| **`FAIL UPDATE`** | Violated constraint causes the pipeline to fail  |\n",
    "| Omitted | Records violating constraints will be kept, and reported in metrics |\n",
    "\n",
    "Notice also that we need to use the `LIVE` prefix to refer to other **DLT** tables.  \n",
    "And for streaming **DLT tables**, we need to use the `STREAM` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING LIVE TABLE orders_cleaned (\n",
    "  CONSTRAINT valid_order_number EXPECT (order_id IS NOT NULL) ON VIOLATION DROP ROW\n",
    ")\n",
    "COMMENT \"The cleaned books orders with valid order_id\"\n",
    "AS\n",
    "  SELECT order_id, quantity, o.customer_id, c.profile:first_name as f_name, c.profile:last_name as l_name,\n",
    "         cast(from_unixtime(order_timestamp, 'yyyy-MM-dd HH:mm:ss') AS timestamp) order_timestamp, o.books,\n",
    "         c.profile:address:country as country\n",
    "  FROM STREAM(LIVE.orders_raw) o\n",
    "  LEFT JOIN LIVE.customers c\n",
    "    ON o.customer_id = c.customer_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold Layer Tables\n",
    "\n",
    "Lastly, we declare the **gold table**, in this case, the `daily number of books per customer in a specific region`.  \n",
    "Here it is China.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH LIVE TABLE cn_daily_customer_books\n",
    "COMMENT \"Daily number of books per customer in China\"\n",
    "AS\n",
    "  SELECT customer_id, f_name, l_name, date_trunc(\"DD\", order_timestamp) order_date, sum(quantity) books_counts\n",
    "  FROM LIVE.orders_cleaned\n",
    "  WHERE country = \"China\"\n",
    "  GROUP BY customer_id, f_name, l_name, date_trunc(\"DD\", order_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Running a Delta Live Table Pipeline\n",
    "\n",
    "Let us see now how to use this notebook to create a new **DLT pipeline**.\n",
    "\n",
    "- To do so, start by navigating to the **Workflows** tab on the sidebar.\n",
    "- Select the **Delta Live Table** tab.\n",
    "- Click **Create Pipeline**.\n",
    "\n",
    "<div  style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../assets/images/Screen-Captures/Workflows - Delta Live Tables tab.jpg\" alt=\"Workflows - Delta Live Tables tab\" style=\"width: 1280px\">\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the Pipeline\n",
    "\n",
    "- Under **General**:\n",
    "  - Fill in a **Pipeline name**, for example, `demo_bookstore`.\n",
    "\n",
    "  - The **Pipeline mode** specifies how the pipeline will be run.  \n",
    "   Triggered pipelines run once and then shut down until the next manual or scheduled updates.  \n",
    "   **Continuous pipelines** will continuously ingest new data as it arrives.\n",
    "   For this demo, let us keep it **Triggered**.\n",
    "\n",
    "- For **Source code**, use the navigator to locate and select the notebook with the delta table definitions, this one.\n",
    "\n",
    "- Under **Destination**:\n",
    "  - For **Storage options**, select **Hive Metastore**.\n",
    "  - In the **Storage location** field, enter a path where the pipeline logs and data files will be stored (`dbfs:/mnt/demo/dlt/demo_bookstore`). \n",
    "   We will explore this directory later.\n",
    "  - In the **Target schema** field, enter a target database name (`demo_bookstore_dlt_db`).\n",
    "\n",
    "<div  style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../assets/images/Screen-Captures/Workflows - Delta Live Tables - Create pipeline 1.jpg\" alt=\"Workflows - Delta Live Tables - Create pipeline 1.jpg\" style=\"width: 1280px\">\n",
    "</div>\n",
    "\n",
    "<div  style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../assets/images/Screen-Captures/Workflows - Delta Live Tables - Create pipeline 2.jpg\" alt=\"Workflows - Delta Live Tables - Create pipeline 2.jpg\" style=\"width: 1280px\">\n",
    "</div>\n",
    "\n",
    "- Under **Compute**:\n",
    "  - A new cluster will be created for our **DLT pipeline**.  \n",
    "   For this, let us choose the **cluster mode**. For example, **fixed size**.\n",
    "  - Set the number of **Workers** to `0` to create a **single-node cluster**.\n",
    "\n",
    "- Under **Advanced**:\n",
    "  - For **Configuration**, add a new configuration parameter.\n",
    "    Set the key to `dataset.path` and the value to the location of the bookstore dataset(`dbfs:/mnt/demo-datasets/bookstore`).  \n",
    "    This parameter is used in the notebook to specify the path to our source data files.\n",
    "  - For **Driver type**, select **Standard_DS3_v2** (4 Cores) type.\n",
    "  \n",
    "Notice right-side the **DBUs estimate** provided, similar to that provided when configuring interactive clusters.  \n",
    "Finally, click **Create**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Pipeline\n",
    "\n",
    "Great! The pipeline has been created.\n",
    "\n",
    "  <div  style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "    <img src=\"../../assets/images/Screen-Captures/Workflows - Delta Live Tables - demo_bookstore (just created).jpg\" alt=\"Workflows - Delta Live Tables - demo_bookstore (just created).jpg\" style=\"width: 1280px\">\n",
    "  </div>\n",
    "\n",
    "- Select **Development** to run the pipeline in **development mode**.  \n",
    "  This mode allows for interactive development by reusing the cluster, compared to creating a new cluster for each run in the production mode.\n",
    "  **Development mode** also disables retries so that we can quickly identify and fix errors.\n",
    "\n",
    "- Now click **Start**.  \n",
    "  The initial run will take several minutes while the cluster is provisioned.\n",
    "\n",
    "  <div  style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "    <img src=\"../../assets/images/Screen-Captures/Workflows - Delta Live Tables - demo_bookstore (start in progress).jpg\" alt=\"\" style=\"width: 1280px\">\n",
    "  </div>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing and Inspecting the Pipeline\n",
    "\n",
    "Great! Our pipeline successfully ran.\n",
    "\n",
    "- Below, we see all the **events** of our running pipeline, either **information**, **warning**, or **errors**.\n",
    "- On the right-hand side, we see all the pipeline details and information related to the cluster.\n",
    "- In the middle, we see the execution flow visualized as a **Directed Acyclic Graph (DAG)**.  \n",
    "  This **DAG** represents the entities involved in the pipeline and the relationships between them.\n",
    "\n",
    "Click on each entity to view a summary, which includes the **run status** and other **metadata summaries**, including the comment we set during the table definition in the notebook.\n",
    "\n",
    "We can also see the **schema** of the table.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Metrics\n",
    "\n",
    "If you select the `orders_cleaned` table, you can notice the results reported in the **data quality section**.  \n",
    "Because this flow has **data expectation declared**, those metrics are extracted here.\n",
    "\n",
    "As you can see, we have no records violating our constraint.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the Pipeline\n",
    "\n",
    "Let us now come back to our notebook for adding another table and see how this change is reflected here.\n",
    "\n",
    "Let us open the notebook of this pipeline by clicking on the link here.  \n",
    "Let us scroll to the end of this notebook and add a new cell.\n",
    "\n",
    "We will add a new table similar to the previous **gold table** declaration.  \n",
    "But this time, instead of China, we will filter for **France**.\n",
    "\n",
    "But let us do something different to see what happens if we remove, for example, the `LIVE` prefix.\n",
    "\n",
    "If we run this cell, the syntax of the query is correct. However, let us see what will happen in our pipeline.\n",
    "\n",
    "Now click **Start** again to rerun our pipeline and examine the updated results.\n",
    "\n",
    "As you can see, this generates an error: `Table or view not found`, because we missed the `LIVE` namespace.  \n",
    "Let us correct this.\n",
    "\n",
    "Okay, here we add again the `LIVE` keyword, and we run the query.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-running the Pipeline\n",
    "\n",
    "Great! The syntax is valid.\n",
    "\n",
    "Let us rerun our pipeline by clicking **Start**.\n",
    "\n",
    "Great! Our pipeline is successfully completed, and we can see now our two gold tables.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exploring Pipeline Logs and Data](./Lecture-30__Delta-Live-Tables-(Hands-On)-2.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
