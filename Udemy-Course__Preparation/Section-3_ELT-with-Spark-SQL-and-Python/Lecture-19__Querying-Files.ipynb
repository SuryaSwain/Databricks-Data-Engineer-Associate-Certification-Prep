{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 19. Querying Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference  \n",
    "- [Microsoft Learn / Azure Databricks documentation / Query databases using JDBC](https://learn.microsoft.com/en-us/azure/databricks/external-data/jdbc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Files Directly\n",
    "\n",
    "To query a file content, we can simply use a `SELECT` statement.\n",
    "\n",
    "`SELECT * FROM` a file format, and we specify the file path. And make special note of the use of backticks and not single quotes around the path.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../../assets/images/Presentation-Images/Querying Files Directly.jpg\" style=\"width:640px\" >\n",
    "</div> \n",
    "\n",
    "This works well with self-describing formats that have well-defined schema like JSON and parquet.\n",
    "However, it is not very useful with non describing formats like CSV.\n",
    "\n",
    "- A path to file could be \n",
    "  - a single file.\n",
    "  - Or we can use a wildcard character to read multiple files simultaneously.\n",
    "  - Or simply reading the whole directory.\n",
    "    Of course, assuming that all of the files in the directory have the same format and schema.\n",
    "\n",
    "- File Format\n",
    "  - Extract a JSON file.\n",
    "\n",
    "    ```sql\n",
    "    SELECT * FROM json.`/path/file_name.json`\n",
    "    ```\n",
    "\n",
    "    As you see, it simply `SELECT * FROM json`, and we specify the path to file around back ticks.\n",
    "\n",
    "  - Extract files as raw strings\n",
    "    When working with text-based files which include JSON, CSV, TSV and TXT format, you can use the `text` format to extract data as raw strings.\n",
    "    \n",
    "    ```sql\n",
    "    SELECT * FROM text.`/path/to/file`\n",
    "    ```\n",
    "\n",
    "    This can be useful when input data could be corrupted.\n",
    "    In this case, we extract the data as raw string and we apply custom text parsing functions to extract values from text files.\n",
    "\n",
    "  - Extract files as raw bytes\n",
    "\n",
    "    And in some cases, we need the binary representation of files content, for example, when dealing with images and unstructured data.\n",
    "    Here we can use simply `binaryFile` as a format.\n",
    "\n",
    "    ```sql\n",
    "    SELECT * FROM binaryFile.`/path/to/file`\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTAS: Registering Tables from Files\n",
    "\n",
    "And usually after extracting data from external data sources, \n",
    "we need to load them into the lakehouse \n",
    "which ensures that all of the benefits of Databricks platform can be fully leveraged. \n",
    "\n",
    "To load data from files into Delta tables, \n",
    "we use CTAS statements, which is \"Create Table As Select\" query.\n",
    "\n",
    "```sql\n",
    "CREATE TABLE table_name\n",
    "AS SELECT * FROM file_format.`/path/to/file`\n",
    "```\n",
    "\n",
    "Here we are querying data from files directly.\n",
    "\n",
    "CTAS statements automatically inferior schema information from query results and do not support manual schema declaration.\n",
    "This means the CTAS statements are useful for external data injection from sources with well-defined schema such as parquet files and tables.\n",
    "\n",
    "### Limitation\n",
    "\n",
    "CTAS statements also do not support specifying additional file options.\n",
    "And this is why this statement presents significant limitation when trying to ingest data from CSV files.\n",
    "For such a format that requires additional options, we need another solution that supports options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Options for External Sources\n",
    "\n",
    "### `CREATE TABLE USING` statement\n",
    "\n",
    "  This solution is the regular `CREATE TABLE` statement, but with the `USING` keyword.\n",
    "  By adding the `USING` keyword, we specify the external data source type, for example CSV format and with any additional options.\n",
    "\n",
    "  And of course, you need to specify a location to where these files are stored.\n",
    "\n",
    "  ```sql\n",
    "  CREATE TABLE table_name\n",
    "              (col_name1 col_type1, ...)\n",
    "  USING data_source_type\n",
    "  OPTIONS (key1 = val1, key2 = val2, ...)\n",
    "  LOCATION = path\n",
    "  ```\n",
    "\n",
    "  That means with this command, we are always creating an external table.\n",
    "  The table here is just a reference to the files.\n",
    "\n",
    "  Unlike with CTAS statements, here there is no data moving during table creation.\n",
    "  We are just pointing to files stored in an external location.\n",
    "\n",
    "  Moreover, these files are kept in its original format, which means we are creating here a non-Delta table.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "- Here is an example of creating a table using CSV external source.\n",
    "\n",
    "  ```sql\n",
    "  CREATE TABLE table_name\n",
    "  (col_name1 col_type1, ...)\n",
    "  USING CSV\n",
    "  OPTIONS (header = \"true\",\n",
    "          delimiter = \";\")\n",
    "  LOCATION = path\n",
    "  ```\n",
    "\n",
    "  So again, it's not a Delta table.\n",
    "\n",
    "  We are pointing to CSV files exist in an external location.\n",
    "\n",
    "  And we are specifying the options for reading the files.\n",
    "  Like the fact that there is a header presents in the files and the delimiter is a semicolon.\n",
    "\n",
    "  And finally, we are providing the location to these CSV files.\n",
    "\n",
    "- Another example is to create a table using JDBC connection to refer to data in an external SQL database. And we provide the necessary options like the connection string, the username and the password for this database and of course, the database table containing the data.\n",
    "\n",
    "  ```sql\n",
    "  CREATE TABLE table_name\n",
    "  (col_name1 col_type1, ...)\n",
    "  USING JDBC\n",
    "  OPTIONS (url = \"jdbc:sqlite://hostname:port\",\n",
    "          dbtable = \"database.table\",\n",
    "          user = \"username\",\n",
    "          password = ”pwd” )\n",
    "  ```\n",
    "\n",
    "#### Limitation\n",
    "\n",
    "And again, a table with external data source has a limitation.\n",
    "It is not a Delta table.\n",
    "  \n",
    "* It means the performance and the features of Delta Lake are no more guaranteed, like time travel feature, and the guarantee that we are always reading the most recent version of the data.\n",
    "\n",
    "* In addition, if you are referring to a huge database table, this also can cause performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for `CREATE TABLE USING` Statement Limitation\n",
    "\n",
    "The solution is simply to \n",
    "create a temporary view, referring to the external data source, \n",
    "and then query this temporary view to create a table using CTAS statements.\n",
    "\n",
    "```sql\n",
    "CREATE TEMP VIEW temp_view_name (col_name1 col_type1, ...)\n",
    "USING data_source\n",
    "OPTIONS (key1 = “val1”, key2 = “val2”, ..., path = “/path/to/files”)\n",
    "\n",
    "CREATE TABLE table_name\n",
    "AS SELECT * FROM temp_view_name\n",
    "```\n",
    "\n",
    "In this way we are extracting the data from the external data source and load it in a Delta table.\n",
    "\n",
    "And as you can see, with CTAS statement, you can not only query files, but you can query any object like a temporary view in this case."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
