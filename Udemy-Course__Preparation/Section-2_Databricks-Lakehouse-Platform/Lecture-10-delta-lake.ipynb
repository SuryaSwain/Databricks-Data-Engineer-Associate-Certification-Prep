{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10. Delta Lake\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Delta lake\n",
    "\n",
    "*Delta lake* is an open source storage framework that brings reliability to data lakes.\n",
    "\n",
    "As you may know, data lakes have many limitations, such as data inconsistency and performance issues.\n",
    "\n",
    "Delta lake technology helps overcoming these challenges.\n",
    "\n",
    "Let us see this comparison to better understand what is Delta lake\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../../assets/images/Presentation-Images/Delta Lake is and isnot.jpg\" style=\"width:640px\" >\n",
    "</div> \n",
    "\n",
    "- As we said, Delta Lake is an open source technology and not a proprietary technology.\n",
    "\n",
    "- It's a storage framework or a storage layer, but it is not a storage format or a storage medium.\n",
    "\n",
    "- It enables building lakehouse architecture.\n",
    "\n",
    "Lakehouse is a platform that unify both data warehouse and advanced analytics.\n",
    "\n",
    "So Delta Lake itself is not a data warehouse, and of course it's not a database service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dalta Lake in more details\n",
    "\n",
    "Delta Lake is a component which is deployed on the cluster as part of the Databricks runtime.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../../assets/images/Presentation-Images/Delta Lake in more details.jpg\" style=\"width:640px\" >\n",
    "</div> \n",
    "\n",
    "If you are creating a Delta Lake table, it gets stored on the storage in one or more data files in parquet format.\n",
    "\n",
    "But along with these files, Delta stores a transaction log as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what is this transaction log ?\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../../assets/images/Presentation-Images/Transaction log (Delta log).jpg\" style=\"width:640px\" >\n",
    "</div> \n",
    "\n",
    "- The *Delta Lake transaction log*, also known as ***Delta Log***, is ordered records of every transaction performed on the table since its creation. \n",
    "\n",
    "- It serves as a single source of truth. \n",
    "  So every time you query the table, Spark checks this transaction log to retrieve the most recent version of the data.\n",
    "\n",
    "- Each committed transaction is recorded in a JSON file.\n",
    "\n",
    "  It contains the operation that has been performed, whether, for example, it's an insert or update and the predicates such as conditions and filters used during this operation in addition to all the files that have been affected because of this operation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us see some concrete examples.\n",
    "\n",
    "  - In this scenario, we have a writer process and a reader process.\n",
    "\n",
    "    <div style=\"text-align: center;\">\n",
    "    <img src=\"../../assets/images/Presentation-Images/Transaction log - Writes and Reads.jpg\" style=\"width:640px\" >\n",
    "    </div> \n",
    "\n",
    "    Once the writer process starts, it stores the Delta Lake table in two data files in a parquet format.\n",
    "\n",
    "    As soon as the writer process finishes writing, it adds the transaction log 000.json into the _delta_log directory. A reader process always starts by reading a transaction log.\n",
    "\n",
    "    In this case, it reads the 000.json transaction log that contains information of the files number 1 and 2. So, it can start reading them.\n",
    "\n",
    "  - In our second scenario, the writer process wants to update a record which presents in the file number 1. \n",
    "\n",
    "    <div style=\"text-align: center;\">\n",
    "    <img src=\"../../assets/images/Presentation-Images/Transaction log - Updates.jpg\" style=\"width:640px\" >\n",
    "    </div> \n",
    "\n",
    "\n",
    "    But in Delta Lake, instead of updating the record in the file itself, it will make a copy of this file and make the necessary updates in the new file, file number 3.\n",
    "\n",
    "    It then updates the log by writing a new JSON file.\n",
    "    This new log file knows that file number 1 is no longer needed.\n",
    "\n",
    "    Now, the reader process reads the transaction log that tells that only files 2 and 3 are part of the current table version so it can start reading them.\n",
    "\n",
    "  - Let us see one more scenario.\n",
    "    Here, both processes want to work at the same time.\n",
    "\n",
    "    <div style=\"text-align: center;\">\n",
    "    <img src=\"../../assets/images/Presentation-Images/Transaction log - Simultaneous Writes and Reads.jpg\" style=\"width:640px\" >\n",
    "    </div> \n",
    "\n",
    "    The writer process starts writing the file number 4.\n",
    "\n",
    "    On the other hand, the reader process reads the transaction log that only has information about files 2 and 3 and not file number 4 as it is not fully written yet.\n",
    "\n",
    "    So it starts reading those two files, 2 and 3 which represent the most recent data at the moment.\n",
    "\n",
    "    So as you can see here, Delta Lake guarantees that you will always get the most recent version of the data.\n",
    "\n",
    "    Your read operation will never have a deadlock state or conflicts with any ongoing operation on the table.\n",
    "\n",
    "    Finally, the writer process finishes and it adds a new file to the log.\n",
    "\n",
    "  - Here is our last scenario.\n",
    "    The writer process starts writing the file number 5 to the lake, but this time there is an error in the job, which leads to adding an incomplete file.\n",
    "\n",
    "    <div style=\"text-align: center;\">\n",
    "    <img src=\"../../assets/images/Presentation-Images/Transaction log - Failed Writes.jpg\" style=\"width:640px\" >\n",
    "    </div> \n",
    "\n",
    "    Because of this failure, Delta Lake module does not write any information to the log.\n",
    "\n",
    "    Now, the reader process reads the transaction log that has no information about that incomplete file number 5.\n",
    "\n",
    "    That's why the reader process will read only files 2, 3 and 4.\n",
    "\n",
    "    So as you can see, Delta Lake guarantees that you will never read dirty data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake Advantages\n",
    "\n",
    "  So the transaction log is the magic behind the scene.\n",
    "\n",
    "  <div style=\"text-align: center;\">\n",
    "  <img src=\"../../assets/images/Presentation-Images/Delta Lake Advantages.jpg\" style=\"width:640px\" >\n",
    "  </div> \n",
    "\n",
    "  * It allows Delta Lake to perform ACID transactions on data lakes.\n",
    "\n",
    "    *ACID transactions* are a set of properties that ensure database transactions are reliable and consistent, even in the face of errors, power failures, or other issues. **ACID** stands for Atomicity, Consistency, Isolation, and Durability\n",
    "\n",
    "  * And it allows also to handle scalable metadata.\n",
    "\n",
    "  * This log also provides the full audit trail of all the changes that have happened on the table.\n",
    "\n",
    "  * And as we saw, the underlying file format for Delta is nothing but parquet and JSON format.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
